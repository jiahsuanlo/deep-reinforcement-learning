{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Project Report\n",
    "\n",
    "Date: 5/12/2019\n",
    "\n",
    "Author: Josh Lo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This report describes the implementation of the learning algorithm used to solve a continuous control problem of a doulbe-joint robot. A deep deterministic policy gradient method was used in this project. There are a target and a local neuro-networks for each of the actor and critic models. The agent uses the experience replay to break the correlations between the samples during training. An Ornstein-Uhlenbeck process was also used to provide some randomness in the generated action. A soft updating scheme using an exponential weighted method was adopted to blend the target network parameters with the local network parameters gradually. The details will be listed in the following sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDPG Agent\n",
    "\n",
    "The DDPG agent include an actor and a critic component. The actor takes the state as input and outputs the action. The output action is then fed into the critic network to generate the state-action Q-value. The neuro-network models of the actor and critic components are listed below:\n",
    "\n",
    "#### Actor Model\n",
    "\n",
    "- Local network: 2-layer of fully-connected with 400 and 300 hidden units in each layer\n",
    "- Target network: 2-layer of fully-connected with 400 and 300 hidden units in each layer\n",
    "\n",
    "For each of the network, a relu layer is used after each linear layer, and a tanh function is used at the output to create a -1 to 1 action output.\n",
    "\n",
    "\n",
    "#### Critic Model\n",
    "\n",
    "- Local network: 2-layer of fully-connected with 400 and 300 hidden units in each layer\n",
    "- Target network: 2-layer of fully-connected with 400 and 300 hidden units in each layer\n",
    "\n",
    "For each of the network, a relu layer is used after each linear layer. The action from the actor network is concatenated with the first-layer output and fed into the second layer.\n",
    "\n",
    "#### Actor Model Update\n",
    "\n",
    "The loss of the actor model is computed as:\n",
    "\n",
    "- $ actions_{pred} = actor_{local}(states) $\n",
    "- $ actor_{loss} = -mean(critic_{local}(states, actions_{pred}))$\n",
    "\n",
    "The negative sign is used to maximize the Q-value.\n",
    "\n",
    "\n",
    "#### Critic Model Update\n",
    "\n",
    "The loss of the critic model is computed as:\n",
    "\n",
    "- $Q_{targets}(s,a)= rewards + \\gamma Q_{targets}(s_{next},a)$\n",
    "- $Q_{locals} = model_{critic}(s,a)$\n",
    "- $loss_{critic}= mean(Q_{targets} - Q_{locals})**2$\n",
    "\n",
    "#### Target Model Soft Update\n",
    "\n",
    "The target model is updated using a soft-update scheme: $\\theta_{target} = \\tau*\\theta_{local} + (1 - \\tau)*\\theta_{target}$\n",
    "\n",
    "#### Noise of Action\n",
    "\n",
    "An Ornstein-Uhlenbeck process was used to introduce noise into the action to facilitate the action. The noise parameters are:\n",
    "\n",
    "- $\\mu = 0.0$\n",
    "- $\\theta= 0.15$\n",
    "- $\\sigma= 0.2$\n",
    "\n",
    "#### Replay Buffer\n",
    "\n",
    "A replay buffer with random sample selection scheme was used to break the correlation between adjacent samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameters \n",
    "\n",
    "The hyper parameters used to train the agent are listed below:\n",
    "\n",
    "```\n",
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 1024     # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-4             # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4       # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "UPDATE_EVERY = 2      # update every x time steps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of Rewards\n",
    "\n",
    "The agent was trained to solve the problem with 425 episodes. Here is the trainning history:\n",
    "\n",
    "```\n",
    "Episode 50      Average Score: 0.83\n",
    "Episode 100     Average Score: 1.60\n",
    "Episode 150     Average Score: 2.76\n",
    "Episode 200     Average Score: 5.60\n",
    "Episode 250     Average Score: 10.91\n",
    "Episode 300     Average Score: 17.61\n",
    "Episode 350     Average Score: 23.11\n",
    "Episode 400     Average Score: 28.22\n",
    "Episode 425     Average Score: 30.04\n",
    "Environment solved in 425 episodes!     Average Score: 30.04\n",
    "```\n",
    "\n",
    "The resulting rewards plot is shown below:\n",
    "\n",
    "![History of Rewards](./images/ScoreHistory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas of Future Work\n",
    "\n",
    "The possible direction for improving the model perfomance may include:\n",
    "\n",
    "- Priority sampling from the replay buffer: using TD error to determine the selection priority may improve the effciency of sample usage.\n",
    "- Incorporating generalized advantage estimation: adding multiple step estimation using TD($\\lambda$) scheme may increase the target estimation accuracy.\n",
    "- Fine tuning the hyper parameters: since the hyper parameters used to train the model is basically based on the default values from the course repository, a more systematic tuning of the parameters may improve the performance.\n",
    "- Trying other deep RL framework, such as A2C, A3C, D4PG etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
